{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli download microsoft/Phi-3-mini-4k-instruct-gguf Phi-3-mini-4k-instruct-q4.gguf --local-dir /kaggle/working/ --local-dir-use-symlinks False","metadata":{"execution":{"iopub.status.busy":"2024-07-21T12:02:12.954720Z","iopub.execute_input":"2024-07-21T12:02:12.955151Z","iopub.status.idle":"2024-07-21T12:03:27.813845Z","shell.execute_reply.started":"2024-07-21T12:02:12.955115Z","shell.execute_reply":"2024-07-21T12:03:27.812480Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/commands/download.py:132: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n  warnings.warn(\nDownloading 'Phi-3-mini-4k-instruct-q4.gguf' to '/kaggle/working/.huggingface/download/Phi-3-mini-4k-instruct-q4.gguf.8a83c7fb9049a9b2e92266fa7ad04933bb53aa1e85136b7b30f1b8000ff2edef.incomplete' (resume from 251658240/2393231072)\nPhi-3-mini-4k-instruct-q4.gguf: 100%|██████| 2.39G/2.39G [01:12<00:00, 29.4MB/s]\nDownload complete. Moving file to /kaggle/working/Phi-3-mini-4k-instruct-q4.gguf\n/kaggle/working/Phi-3-mini-4k-instruct-q4.gguf\n","output_type":"stream"}]},{"cell_type":"code","source":"# there may be some error while installing llama-cpp-python which requires cmake.\n# Below command is fine especially for cpu.","metadata":{"execution":{"iopub.status.busy":"2024-07-21T11:48:56.351758Z","iopub.execute_input":"2024-07-21T11:48:56.352285Z","iopub.status.idle":"2024-07-21T11:48:56.357814Z","shell.execute_reply.started":"2024-07-21T11:48:56.352216Z","shell.execute_reply":"2024-07-21T11:48:56.356461Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"! CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python==0.2.81 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu","metadata":{"execution":{"iopub.status.busy":"2024-07-21T12:03:52.815121Z","iopub.execute_input":"2024-07-21T12:03:52.815677Z","iopub.status.idle":"2024-07-21T12:04:05.498553Z","shell.execute_reply.started":"2024-07-21T12:03:52.815577Z","shell.execute_reply":"2024-07-21T12:04:05.497127Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cpu\nRequirement already satisfied: llama-cpp-python==0.2.81 in /opt/conda/lib/python3.10/site-packages (0.2.81)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python==0.2.81) (4.9.0)\nRequirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python==0.2.81) (1.26.4)\nRequirement already satisfied: diskcache>=5.6.1 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python==0.2.81) (5.6.3)\nRequirement already satisfied: jinja2>=2.11.3 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python==0.2.81) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.81) (2.1.3)\n","output_type":"stream"}]},{"cell_type":"code","source":"! apt-get install -y musl-dev\n! ln -s /usr/lib/x86_64-linux-musl/libc.so /lib/libc.musl-x86_64.so.1","metadata":{"execution":{"iopub.status.busy":"2024-07-21T12:04:05.500828Z","iopub.execute_input":"2024-07-21T12:04:05.501214Z","iopub.status.idle":"2024-07-21T12:04:09.977467Z","shell.execute_reply.started":"2024-07-21T12:04:05.501179Z","shell.execute_reply":"2024-07-21T12:04:09.976038Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nmusl-dev is already the newest version (1.1.24-1).\n0 upgraded, 0 newly installed, 0 to remove and 69 not upgraded.\nln: failed to create symbolic link '/lib/libc.musl-x86_64.so.1': File exists\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Run the model","metadata":{}},{"cell_type":"code","source":"from llama_cpp import Llama\n\n\nllm = Llama(\n  model_path=\"./Phi-3-mini-4k-instruct-q4.gguf\",  # path to GGUF file\n  n_ctx=4096,  # The max sequence length to use - note that longer sequence lengths require much more resources\n  n_threads=8, # The number of CPU threads to use, tailor to your system and the resulting performance\n  n_gpu_layers=35, # The number of layers to offload to GPU, if you have GPU acceleration available. Set to 0 if no GPU acceleration is available on your system.\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T12:04:12.091660Z","iopub.execute_input":"2024-07-21T12:04:12.092154Z","iopub.status.idle":"2024-07-21T12:04:13.917869Z","shell.execute_reply.started":"2024-07-21T12:04:12.092113Z","shell.execute_reply":"2024-07-21T12:04:13.916683Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"llama_model_loader: loaded meta data with 24 key-value pairs and 195 tensors from ./Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = phi3\nllama_model_loader: - kv   1:                               general.name str              = Phi3\nllama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\nllama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\nllama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\nllama_model_loader: - kv   5:                           phi3.block_count u32              = 32\nllama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\nllama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\nllama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\nllama_model_loader: - kv  10:                          general.file_type u32              = 15\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\nllama_model_loader: - kv  23:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_K:   81 tensors\nllama_model_loader: - type q5_K:   32 tensors\nllama_model_loader: - type q6_K:   17 tensors\nllm_load_vocab: special tokens cache size = 323\nllm_load_vocab: token to piece cache size = 0.1687 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = phi3\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32064\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 3072\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 96\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 96\nllm_load_print_meta: n_embd_head_v    = 96\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 3072\nllm_load_print_meta: n_embd_v_gqa     = 3072\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 8192\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 2\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 3B\nllm_load_print_meta: model ftype      = Q4_K - Medium\nllm_load_print_meta: model params     = 3.82 B\nllm_load_print_meta: model size       = 2.23 GiB (5.01 BPW) \nllm_load_print_meta: general.name     = Phi3\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOT token        = 32007 '<|end|>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.10 MiB\nllm_load_tensors:        CPU buffer size =  2281.66 MiB\n...........................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB\nllama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB\nllama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\nllama_new_context_with_model:        CPU compute buffer size =   300.01 MiB\nllama_new_context_with_model: graph nodes  = 1286\nllama_new_context_with_model: graph splits = 1\nAVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \nModel metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.bos_token_id': '1', 'general.architecture': 'phi3', 'phi3.context_length': '4096', 'phi3.attention.head_count_kv': '32', 'general.name': 'Phi3', 'tokenizer.ggml.pre': 'default', 'phi3.embedding_length': '3072', 'tokenizer.ggml.unknown_token_id': '0', 'phi3.feed_forward_length': '8192', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.block_count': '32', 'phi3.attention.head_count': '32', 'phi3.rope.dimension_count': '96', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\nAvailable chat formats from metadata: chat_template.default\nUsing gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n' + message['content'] + '<|end|>' + '\n' + '<|assistant|>' + '\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n'}}{% endif %}{% endfor %}\nUsing chat eos_token: <|endoftext|>\nUsing chat bos_token: <s>\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = \"How to explain 2+3 = 5?\"\n\n# Simple inference example\noutput = llm(\n  f\"<|user|>\\n{prompt}<|end|>\\n<|assistant|>\",\n  max_tokens=256,  # Generate up to 256 tokens\n  stop=[\"<|end|>\"], \n  echo=True,  # Whether to echo the prompt\n)\n\nprint(output['choices'][0]['text'])","metadata":{"execution":{"iopub.status.busy":"2024-07-21T12:05:48.141527Z","iopub.execute_input":"2024-07-21T12:05:48.141952Z","iopub.status.idle":"2024-07-21T12:07:00.002515Z","shell.execute_reply.started":"2024-07-21T12:05:48.141916Z","shell.execute_reply":"2024-07-21T12:07:00.000165Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"Llama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1644.52 ms\nllama_print_timings:      sample time =     169.45 ms /   256 runs   (    0.66 ms per token,  1510.74 tokens per second)\nllama_print_timings: prompt eval time =    1216.04 ms /    10 tokens (  121.60 ms per token,     8.22 tokens per second)\nllama_print_timings:        eval time =   70205.56 ms /   255 runs   (  275.32 ms per token,     3.63 tokens per second)\nllama_print_timings:       total time =   71846.79 ms /   265 tokens\n","output_type":"stream"},{"name":"stdout","text":"<|user|>\nHow to explain 2+3 = 5?<|end|>\n<|assistant|> In mathematics, the equation \"2 + 3 = 5\" is incorrect according to standard arithmetic rules. However, I can help you understand a context where this might be used metaphorically or in a non-standard system:\n\n1. **Metaphorical Context**: Sometimes, equations like \"2 + 3 = 5\" are used metaphorically to suggest that combining two items (represented by '2' and '3') somehow results in an additional three units ('5'). This might be seen in situations where you consider not just the immediate sum but also external factors contributing to the total. For example, \"If I have 2 apples (2) and find another 3 oranges (3), my fruit collection increases by 5.\"\n\n2. **Non-standard Arithmetic Systems**: There are alternative number systems where the rules differ from standard arithmetic. However, these typically aren't used for basic calculations but rather as a way to explore different mathematical concepts. An example is modular arithmetic, which wraps around after reaching a certain value (modulus). In some interpretations of modulo 4 arithmetic, \"2 + 3 = 1\" because when you add \n","output_type":"stream"}]},{"cell_type":"code","source":"# It takes around 1 min to generate the response, with CPU only.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}